import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { AgentOrchestrator } from '@/lib/ai/agents';
import { UserProfileStorage } from '@/lib/ai/storage';
import { createApiLogger } from '@/lib/logger';
import { handleApiError } from '@/lib/errors';

const AI_PROVIDER = process.env.AI_PROVIDER || 'openai'; // 'openai' or 'claude'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// Only initialize Gemini if API key is available
const geminiApiKey = process.env.NEXT_PUBLIC_GEMINI_API_KEY || process.env.GEMINI_API_KEY;
const gemini = geminiApiKey ? new GoogleGenerativeAI(geminiApiKey) : null;

// Perplexity for real-time data
const perplexityApiKey = process.env.NEXT_PUBLIC_PERPLEXITY_API_KEY || process.env.PERPLEXITY_API_KEY;
const perplexity = perplexityApiKey ? new OpenAI({
  apiKey: perplexityApiKey,
  baseURL: 'https://api.perplexity.ai'
}) : null;

const SYSTEM_PROMPT = `You are Homie, a helpful AI assistant for HomieHouse - a Farcaster-based social platform. 

Key Facts about Farcaster:
- Farcaster was created by Dan Romero and Varun Srinivasan
- It's a sufficiently decentralized social network protocol
- Built on Ethereum/Optimism blockchain
- Users own their social graph and data
- Accounts are identified by FIDs (Farcaster IDs)

Key Facts about HomieHouse:
- A social hub built on Farcaster protocol
- Users can view feeds, compose casts, and interact with the Farcaster network
- Integrates with Farcaster AuthKit for authentication

Your Capabilities:
- Analyze casts that users are viewing (cast context will be provided in the conversation)
- Explain tokens, crypto concepts, and blockchain technology
- Provide context about users and trends
- Analyze sentiment and engagement metrics
- Explain technical terms and abbreviations
- Identify potential scams or risky content
- Help users understand what they're seeing in their feed

IMPORTANT - Cast Context:
- When cast context is provided, it will be clearly marked at the start of the conversation
- The cast details include: author, content, timestamp, and engagement metrics
- When users ask questions like "what do you think?", "analyze this", "is this legit?", or ask about finding similar casts, they are referring to the provided cast context
- You CANNOT search for other casts or access the Farcaster network directly
- If asked to find similar casts, explain that you can analyze the provided cast in detail but cannot search the network for similar ones

When analyzing a cast:
- Look at the content, author details, and engagement metrics provided
- Identify the main topic or purpose (e.g., token launch, announcement, question, meme)
- Note any red flags (suspicious addresses, low engagement accounts, scam indicators)
- Provide clear, actionable insights

Be friendly, concise, and accurate. If you're not certain about something, say so rather than guessing.`;

// Intelligent provider selection based on question content
function selectBestProvider(question: string): 'perplexity' | 'gemini' | 'claude' | 'openai' {
  const lowerQuestion = question.toLowerCase();
  
  // Keywords that suggest Perplexity would be best (real-time data)
  const perplexityKeywords = [
    'latest', 'current', 'now', 'today', 'tonight', 'recent', 'just',
    'live', 'right now', 'this week', 'this month', 'this year',
    'price', 'score', 'weather', 'news', 'breaking', 'update',
    'happening', 'trending', 'what happened', 'did', 'market',
    'stock', 'crypto', 'bitcoin', 'eth', 'when did', 'who won'
  ];
  
  // Keywords that suggest Gemini with web search would be better
  const geminiKeywords = [
    'search', 'find', 'what is', 'explain', 'tell me about', 
    'link', 'url', 'website', 'https://', 'http://', '.com', 
    'article', 'documentation', 'guide', 'tutorial', 'how to'
  ];
  
  // Keywords that suggest Claude would be better
  const claudeKeywords = [
    'write', 'story', 'creative', 'essay', 'poem', 'philosophical',
    'explain like', 'eli5', 'analogy', 'metaphor', 'analyze',
    'opinion', 'think about', 'perspective', 'nuance', 'ethics',
    'summarize', 'rewrite', 'improve', 'feedback', 'critique'
  ];
  
  // Keywords that suggest OpenAI would be better
  const openaiKeywords = [
    'code', 'function', 'debug', 'error', 'api', 'typescript',
    'javascript', 'python', 'sql', 'algorithm', 'data structure',
    'calculate', 'math', 'number', 'formula', 'json',
    'parse', 'regex', 'syntax', 'compile', 'implement'
  ];
  
  let perplexityScore = 0;
  let geminiScore = 0;
  let claudeScore = 0;
  let openaiScore = 0;
  
  // Count keyword matches
  for (const keyword of perplexityKeywords) {
    if (lowerQuestion.includes(keyword)) perplexityScore++;
  }
  
  for (const keyword of geminiKeywords) {
    if (lowerQuestion.includes(keyword)) geminiScore++;
  }
  
  for (const keyword of claudeKeywords) {
    if (lowerQuestion.includes(keyword)) claudeScore++;
  }
  
  for (const keyword of openaiKeywords) {
    if (lowerQuestion.includes(keyword)) openaiScore++;
  }
  
  // Check for URLs in the question - strong signal for Gemini
  if (/https?:\/\/[^\s]+/.test(question)) {
    geminiScore += 3;
  }
  
  // Additional heuristics
  if (lowerQuestion.includes('?') && lowerQuestion.split(' ').length > 15) {
    // Long questions tend to be better for Claude's analysis
    claudeScore++;
  }
  
  if (lowerQuestion.includes('```') || lowerQuestion.includes('function') || lowerQuestion.includes('const')) {
    // Code blocks suggest technical question
    openaiScore += 2;
  }
  
  // Return provider with highest score
  if (perplexityScore > 0 && perplexity) return 'perplexity';
  if (geminiScore > claudeScore && geminiScore > openaiScore) return 'gemini';
  if (openaiScore > claudeScore) return 'openai';
  return 'claude'; // Default to Claude for general questions
}

// Detect if user is asking for a profile and extract username
function detectProfileRequest(question: string): string | null {
  const lowerQuestion = question.toLowerCase();
  
  // Profile request patterns
  const patterns = [
    /profile (?:of|for) @?(\w+)/i,
    /who is @?(\w+)/i,
    /tell me about @?(\w+)/i,
    /info (?:on|about) @?(\w+)/i,
    /pull (?:the )?profile (?:of|for) @?(\w+)/i,
    /get (?:the )?profile (?:of|for) @?(\w+)/i,
    /show me @?(\w+)(?:'s| )?(?:profile)?/i,
    /look up @?(\w+)/i,
    /find @?(\w+)(?:'s| )?(?:profile)?/i,
  ];
  
  for (const pattern of patterns) {
    const match = question.match(pattern);
    if (match && match[1]) {
      return match[1];
    }
  }
  
  // Check for standalone @mention or username in question
  const mentionMatch = question.match(/@(\w+)/);
  if (mentionMatch && (lowerQuestion.includes('profile') || lowerQuestion.includes('who'))) {
    return mentionMatch[1];
  }
  
  return null;
}

// Extract cast hash or URL from question
function extractCastReference(question: string): string | null {
  // Match cast URLs: https://warpcast.com/username/0x123...
  const warpcastUrlMatch = question.match(/warpcast\.com\/\w+\/(0x[a-fA-F0-9]+)/);
  if (warpcastUrlMatch) {
    return warpcastUrlMatch[1];
  }
  
  // Match direct hash references: 0x followed by hex
  const hashMatch = question.match(/(0x[a-fA-F0-9]{8,})/);
  if (hashMatch) {
    return hashMatch[1];
  }
  
  // Match HomieHouse cast URLs: /cast/0x123...
  const homieUrlMatch = question.match(/\/cast\/(0x[a-fA-F0-9]+)/);
  if (homieUrlMatch) {
    return homieUrlMatch[1];
  }
  
  return null;
}

// Fetch cast data from Neynar API
async function fetchCastData(castHash: string) {
  try {
    const apiKey = process.env.NEYNAR_API_KEY;
    if (!apiKey) {
      console.error('NEYNAR_API_KEY not configured');
      return null;
    }

    const url = `https://api.neynar.com/v2/farcaster/cast?identifier=${castHash}&type=hash`;
    console.log(`Fetching cast ${castHash} from Neynar API`);
    
    const response = await fetch(url, {
      headers: {
        'accept': 'application/json',
        'api_key': apiKey,
      },
    });
    
    if (!response.ok) {
      console.error(`Cast fetch failed: ${response.status} ${response.statusText}`);
      return null;
    }
    
    const data = await response.json();
    console.log(`Cast fetched successfully: ${castHash}`);
    return data.cast;
  } catch (error) {
    console.error('Error fetching cast:', error);
    return null;
  }
}

// Fetch profile directly from Neynar API
async function fetchUserProfile(username: string) {
  try {
    const apiKey = process.env.NEYNAR_API_KEY;
    if (!apiKey) {
      console.error('NEYNAR_API_KEY not configured');
      return null;
    }

    const url = `https://api.neynar.com/v2/farcaster/user/by_username?username=${username}`;
    console.log(`Fetching profile for ${username} from Neynar API`);
    
    const response = await fetch(url, {
      headers: {
        'accept': 'application/json',
        'api_key': apiKey,
      },
    });
    
    if (!response.ok) {
      console.error(`Profile fetch failed: ${response.status} ${response.statusText}`);
      return null;
    }
    
    const data = await response.json();
    console.log(`Profile fetched successfully for ${username}`);
    return data.user;
  } catch (error) {
    console.error('Error fetching profile:', error);
    return null;
  }
}

export async function POST(req: NextRequest) {
  const logger = createApiLogger('/ask-homie');
  logger.start();

  try {
    const { 
      messages, 
      provider: requestedProvider, 
      castContext,
      mode = 'agent',
      userId,
      intent,
      feedback
    } = await req.json();

    logger.info('Processing AI request', { 
      mode, 
      provider: requestedProvider, 
      hasCastContext: !!castContext,
      messageCount: messages?.length 
    });

    // Get or create user ID (from auth or generate temporary)
    const userIdentifier = userId || `temp_${Date.now()}`;

    // If using new agent mode
    if (mode === 'agent') {
      try {
        // Get user profile
        const userProfile = UserProfileStorage.getProfile(userIdentifier);
        
        // Create orchestrator
        const orchestrator = new AgentOrchestrator(userProfile);

        // Get the last user message
        const lastUserMessage = [...messages].reverse().find((m: any) => m.role === 'user');
        const userMessage = lastUserMessage?.content || '';

        // Log cast context if present
        if (castContext) {
          console.log('ðŸ“‹ Cast context received:', {
            author: castContext.author?.username || castContext.author,
            textPreview: castContext.text?.slice(0, 100),
            fullContext: castContext
          });
        }

        // Check if user is asking about a cast (by URL or hash)
        const castHash = extractCastReference(userMessage);
        let fetchedCastData = null;
        
        if (castHash) {
          console.log(`Cast reference detected in agent mode: ${castHash}`);
          fetchedCastData = await fetchCastData(castHash);
        }

        // Use fetched cast data if available, otherwise use provided castContext
        const activeCastContext = fetchedCastData || castContext;

        // Add cast context if provided - make it more prominent
        let contextualMessage = userMessage;
        if (activeCastContext) {
          const castDetails = `
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“‹ CAST BEING ANALYZED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ‘¤ Author: @${activeCastContext.author?.username || activeCastContext.author}
   Display Name: ${activeCastContext.author?.display_name || activeCastContext.author}
   FID: ${activeCastContext.author?.fid || 'N/A'}

ðŸ’¬ Cast Content:
"${activeCastContext.text}"

ðŸ“Š Engagement:
   â¤ï¸  ${activeCastContext.reactions?.likes_count || 0} likes
   ðŸ”„ ${activeCastContext.reactions?.recasts_count || 0} recasts  
   ðŸ’­ ${activeCastContext.replies?.count || 0} replies

ðŸ•’ Posted: ${activeCastContext.timestamp ? new Date(activeCastContext.timestamp).toLocaleString() : 'N/A'}
ðŸ”— Hash: ${activeCastContext.hash || 'N/A'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ‘¤ User's Question: ${userMessage}

ANALYZE THE CAST ABOVE. When the user asks about "this cast", "annaramirez", or asks to find similar casts, they are referring to the cast shown above. Note: You cannot search for other casts, but you can provide detailed analysis of this one.`;
          
          contextualMessage = castDetails;
        }

        // Process with feedback if provided
        if (feedback) {
          UserProfileStorage.addFeedback(userIdentifier, feedback.cast, feedback.feedback);
        }

        // Process the request
        const result = await orchestrator.processRequest(contextualMessage, intent || 'auto');

        // If this was a composition, save it
        if (result.role === 'composer' && result.suggestions && result.suggestions.length > 0) {
          result.suggestions.forEach(cast => {
            UserProfileStorage.addCast(userIdentifier, cast);
          });
        }

        return NextResponse.json({
          response: result.content,
          suggestions: result.suggestions,
          agentRole: result.role,
          metadata: result.metadata,
          mode: 'agent',
          userStats: UserProfileStorage.getStats(userIdentifier)
        });
      } catch (agentError) {
        console.error('Agent mode error, falling back to legacy:', agentError);
        // Fall through to legacy mode
      }
    }

    // Legacy mode (original implementation)
    // Get the last user message to analyze
    const lastUserMessage = [...messages].reverse().find((m: any) => m.role === 'user');
    const question = lastUserMessage?.content || '';

    // Check if this is a profile request
    const requestedUsername = detectProfileRequest(question);
    let profileData = null;
    
    if (requestedUsername) {
      console.log(`Profile request detected for: ${requestedUsername}`);
      profileData = await fetchUserProfile(requestedUsername);
    }

    // Check if user is asking about a cast (by URL or hash)
    const castHash = extractCastReference(question);
    let fetchedCastData = null;
    
    if (castHash) {
      console.log(`Cast reference detected: ${castHash}`);
      fetchedCastData = await fetchCastData(castHash);
    }

    // If cast context is provided, prepend it to the conversation
    let conversationMessages = messages;
    
    // Use fetched cast data if available, otherwise use provided castContext
    const activeCastContext = fetchedCastData || castContext;
    
    if (activeCastContext) {
      const contextMessage = {
        role: 'system',
        content: `â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“‹ CAST BEING ANALYZED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The user is analyzing a Farcaster cast with these details:

ðŸ‘¤ Author: @${activeCastContext.author?.username || activeCastContext.author}
   Display Name: ${activeCastContext.author?.display_name || activeCastContext.author}
   FID: ${activeCastContext.author?.fid || 'N/A'}

ðŸ’¬ Content: "${activeCastContext.text}"

ðŸ“Š Engagement:
   â¤ï¸  ${activeCastContext.reactions?.likes_count || 0} likes
   ðŸ”„ ${activeCastContext.reactions?.recasts_count || 0} recasts
   ðŸ’­ ${activeCastContext.replies?.count || 0} replies

ðŸ•’ Timestamp: ${activeCastContext.timestamp ? new Date(activeCastContext.timestamp).toLocaleString() : 'N/A'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

When the user asks questions like "what do you think?", "analyze this", "is this legit?", or asks to find similar casts, they are referring to THIS cast above. 

IMPORTANT: You cannot search for other casts on Farcaster. If asked to find similar casts, explain that you can analyze this specific cast in detail but cannot search the network for others. Focus on analyzing the content, author credibility, engagement patterns, and any red flags.`
      };
      conversationMessages = [contextMessage, ...messages];
    }

    // If we have profile data, inject it into the conversation
    if (profileData) {
      const verifiedAddresses = profileData.verified_addresses?.eth_addresses || [];
      const profileContext = {
        role: 'user',
        content: `[PROFILE DATA for @${profileData.username}:
Username: ${profileData.username}
Display Name: ${profileData.display_name || 'N/A'}
FID: ${profileData.fid}
Bio: ${profileData.profile?.bio?.text || 'No bio'}
Followers: ${profileData.follower_count || 0}
Following: ${profileData.following_count || 0}
Verified Addresses: ${verifiedAddresses.length > 0 ? verifiedAddresses.join(', ') : 'None'}
Profile URL: https://warpcast.com/${profileData.username}]`
      };
      conversationMessages = [profileContext, ...conversationMessages];
    }

    // Smart provider selection: use requested provider, or auto-select based on content
    const selectedProvider = requestedProvider || selectBestProvider(question);

    let response: string;
    let usedProvider = selectedProvider;

    try {
      if (selectedProvider === 'perplexity' && perplexity) {
        // Use Perplexity for real-time data
        console.log('Using Perplexity for real-time data');
        const completion = await perplexity.chat.completions.create({
          model: 'sonar',
          messages: [
            {
              role: 'system',
              content: SYSTEM_PROMPT
            },
            ...conversationMessages
          ],
          temperature: 0.2,
          max_tokens: 1024,
        });
        response = completion.choices[0].message.content || '';
      } else if (selectedProvider === 'perplexity' && !perplexity) {
        // Fallback to Gemini if Perplexity not available
        console.log('Perplexity not available, falling back to Gemini');
        usedProvider = 'gemini';
        if (gemini) {
          const model = gemini.getGenerativeModel({ 
            model: 'gemini-2.0-flash-exp',
            systemInstruction: SYSTEM_PROMPT
          });
          const geminiHistory = conversationMessages
            .filter((msg: any) => msg.role !== 'system')
            .map((msg: any) => ({
              role: msg.role === 'assistant' ? 'model' : 'user',
              parts: [{ text: msg.content }]
            }));
          const result = await model.generateContent({
            contents: geminiHistory,
            generationConfig: {
              temperature: 0.8,
              maxOutputTokens: 1024,
            }
          });
          response = result.response.text();
        } else {
          throw new Error('No AI provider available');
        }
      } else if (selectedProvider === 'gemini' && gemini) {
        // Use Gemini with web search capability
        console.log('Using Gemini for web-enhanced response');
        const model = gemini.getGenerativeModel({ 
          model: 'gemini-2.0-flash-exp',
          systemInstruction: SYSTEM_PROMPT
        });
        
        // Build conversation history for Gemini
        const geminiHistory = conversationMessages
          .filter((msg: any) => msg.role !== 'system')
          .map((msg: any) => ({
            role: msg.role === 'assistant' ? 'model' : 'user',
            parts: [{ text: msg.content }]
          }));
        
        const result = await model.generateContent({
          contents: geminiHistory,
          generationConfig: {
            temperature: 0.8,
            maxOutputTokens: 1024,
          }
        });
        
        response = result.response.text();
      } else if (selectedProvider === 'gemini' && !gemini) {
        // Fallback to Claude if Gemini is not available
        console.log('Gemini not available, falling back to Claude');
        const completion = await anthropic.messages.create({
          model: 'claude-3-5-sonnet-20241022',
          max_tokens: 1024,
          system: SYSTEM_PROMPT,
          messages: conversationMessages.map((msg: any) => ({
            role: msg.role === 'system' ? 'user' : msg.role,
            content: msg.content,
          })),
        });

        response = completion.content[0].type === 'text' ? completion.content[0].text : '';
        usedProvider = 'claude';
      } else if (selectedProvider === 'claude') {
        // Use Claude (Anthropic)
        const completion = await anthropic.messages.create({
          model: 'claude-3-5-sonnet-20241022',
          max_tokens: 1024,
          system: SYSTEM_PROMPT,
          messages: conversationMessages.map((msg: any) => ({
            role: msg.role === 'system' ? 'user' : msg.role,
            content: msg.content,
          })),
        });

        response = completion.content[0].type === 'text' ? completion.content[0].text : '';
      } else {
        // Use OpenAI
        const completion = await openai.chat.completions.create({
          model: 'gpt-4o',
          messages: [
            {
              role: 'system',
              content: SYSTEM_PROMPT
            },
            ...conversationMessages
          ],
          temperature: 0.7,
          max_tokens: 500,
        });

        response = completion.choices[0].message.content || '';
      }
    } catch (primaryError) {
      // Fallback to alternate provider if primary fails
      console.warn(`${selectedProvider} failed, trying fallback provider:`, primaryError);
      
      const fallbackProvider = selectedProvider === 'claude' ? 'openai' : 'claude';
      usedProvider = `${fallbackProvider} (fallback)`;

      if (fallbackProvider === 'claude') {
        const completion = await anthropic.messages.create({
          model: 'claude-3-5-sonnet-20241022',
          max_tokens: 1024,
          system: SYSTEM_PROMPT,
          messages: messages.map((msg: any) => ({
            role: msg.role === 'system' ? 'user' : msg.role,
            content: msg.content,
          })),
        });

        response = completion.content[0].type === 'text' ? completion.content[0].text : '';
      } else {
        const completion = await openai.chat.completions.create({
          model: 'gpt-4o',
          messages: [
            {
              role: 'system',
              content: SYSTEM_PROMPT
            },
            ...messages
          ],
          temperature: 0.7,
          max_tokens: 500,
        });

        response = completion.choices[0].message.content || '';
      }
    }

    logger.success('AI response generated', { provider: usedProvider, mode: 'legacy' });
    logger.end();
    return NextResponse.json({ response, provider: usedProvider, mode: 'legacy' });
  } catch (error: any) {
    logger.error('AI request failed', error);
    return handleApiError(error, 'POST /ask-homie');
  }
}

// New endpoint for profile management
export async function PATCH(req: NextRequest) {
  const logger = createApiLogger('/ask-homie PATCH');
  logger.start();

  try {
    const { userId, updates } = await req.json();
    
    if (!userId) {
      return NextResponse.json({ error: 'userId required' }, { status: 400 });
    }

    logger.info('Updating user profile', { userId: userId.substring(0, 8) });

    const updatedProfile = UserProfileStorage.updateProfile(userId, updates);
    const stats = UserProfileStorage.getStats(userId);

    logger.success('Profile updated');
    logger.end();
    return NextResponse.json({
      profile: updatedProfile,
      stats
    });
  } catch (error: any) {
    logger.error('Profile update failed', error);
    return handleApiError(error, 'PATCH /ask-homie');
  }
}

// Get user stats
export async function GET(req: NextRequest) {
  const logger = createApiLogger('/ask-homie GET');
  logger.start();

  try {
    const { searchParams } = new URL(req.url);
    const userId = searchParams.get('userId');
    
    if (!userId) {
      return NextResponse.json({ error: 'userId required' }, { status: 400 });
    }

    logger.info('Fetching user profile', { userId: userId.substring(0, 8) });

    const profile = UserProfileStorage.getProfile(userId);
    const stats = UserProfileStorage.getStats(userId);

    logger.success('Profile fetched');
    logger.end();
    return NextResponse.json({
      profile,
      stats
    });
  } catch (error: any) {
    logger.error('Profile fetch failed', error);
    return handleApiError(error, 'GET /ask-homie');
  }
}
